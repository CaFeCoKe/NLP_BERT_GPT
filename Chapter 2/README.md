# Chapter 2. Tokenization (토큰화)
- Tokenization (토큰화) : 문장을 토큰 시퀀스로 나누는 과정, 토큰화를 수행하는 프로그램을 Tokenizer(토크나이저)라고 한다.<br><br>

  - 단어 단위 토큰화 : 단어(어절)단위로 토큰화를 하는것이다. 가장 쉽게 공백을 이용하여 분리 시키는 방법이 있다.<br><br>
  ![tokenize](https://user-images.githubusercontent.com/86700191/160754729-77ccdaa5-a276-49a7-a9b9-0f821ecbce47.png) <br>
  다만 표현이 살짝만 바뀌어도 따로 어휘집합(Vocabulary)에 포함이 되야 하기 때문에 경우의 수가 많아질 수 있다는 단점이 있다. 
  이를 보완하기 위해 의미있는 단어로 토큰화를 진행하면 공백으로 토큰화 하는 것보다는 경우의 수를 줄여 어휘집합의 크기가 조금이나마 적어진다.<br><br>
  ![tokenize01](https://user-images.githubusercontent.com/86700191/160758723-1459f370-831e-432a-8d99-72a54c8b0427.png)

  - 문자 단위 토큰화 : 문자 단위로 토큰화를 하는것이다. 모든 문자를 어휘집합에 포함하므로 신조어와 같은 미등록 토큰에 대해 자유롭다.
  하지만 의미를 부여하여 토큰화 한것이 아니기 때문에 각 문자 토큰은 의미 구분을 할 수 없다는 단점이 있다. 또한, 토큰 시퀀스가 길어지기 때문에 성능이 떨어질 가능성이 있다.<br><br>
  ![tokenize02](https://user-images.githubusercontent.com/86700191/160758726-fa59bde1-036c-49be-96a9-af98c12def88.png)

  - 서브원드 단위 토큰화 : 단어와 문자 단위 토큰화의 중간에 위치한 토큰화기법이다. 대표적으로 바이트 페어 인코딩이 있다.
  
- Byte Pair Encoding (BPE : 바이트 페어 인코딩) : 정보 압축 알고리즘으로 데이터에서 가장 많이 등장한 문자열을 병합하여 압축하는 기법이다. 
GPT 모델에서 사용하는 토큰화 기법이다. 어휘집합의 크기 증가를 억제 하면서 데이터의 길이를 효율적으로 압축할 수 있다.