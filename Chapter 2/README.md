# Chapter 2. Tokenization (토큰화)
- Tokenization (토큰화) : 문장을 토큰 시퀀스로 나누는 과정, 토큰화를 수행하는 프로그램을 Tokenizer(토크나이저)라고 한다.<br><br>

  - 단어 단위 토큰화 : 단어(어절)단위로 토큰화를 하는것이다. 가장 쉽게 공백을 이용하여 분리 시키는 방법이 있다.<br><br>
  ![tokenize](https://user-images.githubusercontent.com/86700191/160754729-77ccdaa5-a276-49a7-a9b9-0f821ecbce47.png) <br>
  다만 표현이 살짝만 바뀌어도 따로 어휘집합(Vocabulary)에 포함이 되야 하기 때문에 경우의 수가 많아질 수 있다는 단점이 있다. 
  이를 보완하기 위해 의미있는 단어로 토큰화를 진행하면 공백으로 토큰화 하는 것보다는 경우의 수를 줄여 어휘집합의 크기가 조금이나마 적어진다.<br><br>
  ![tokenize01](https://user-images.githubusercontent.com/86700191/160758723-1459f370-831e-432a-8d99-72a54c8b0427.png)

  - 문자 단위 토큰화 : 문자 단위로 토큰화를 하는것이다. 모든 문자를 어휘집합에 포함하므로 신조어와 같은 미등록 토큰에 대해 자유롭다.
  하지만 의미를 부여하여 토큰화 한것이 아니기 때문에 각 문자 토큰은 의미 구분을 할 수 없다는 단점이 있다. 또한, 토큰 시퀀스가 길어지기 때문에 성능이 떨어질 가능성이 있다.<br><br>
  ![tokenize02](https://user-images.githubusercontent.com/86700191/160758726-fa59bde1-036c-49be-96a9-af98c12def88.png)

  - 서브워드 단위 토큰화 : 단어와 문자 단위 토큰화의 중간에 위치한 토큰화기법이다. 대표적으로 바이트 페어 인코딩이 있다.<br><br>
  
- Byte Pair Encoding (BPE : 바이트 페어 인코딩) : 정보 압축 알고리즘으로 데이터에서 가장 많이 등장한 문자열을 병합하여 압축하는 기법이다. 
GPT 모델에서 사용하는 토큰화 기법이다. 어휘집합의 크기 증가를 억제 하면서 데이터의 길이를 효율적으로 압축할 수 있다.<br><br>
![tokenize03](https://user-images.githubusercontent.com/86700191/160821105-8ac772f9-abe5-40b0-b15a-a3368ac524fa.png)
<br><br>
  - BPE를 이용한 어휘집합 구축 : 문장에 프리토크나이즈(Pre-tokenize)를 실시한 다음 문자 단위로 토큰화한다. 토큰은 2개씩 묶어 바이그램 쌍으로 만들어 준다음 가장 등장 빈도가 많은 바이그램 쌍을 병합하는 방식으로 구축한다. <br><br>
  ![BPE01](https://user-images.githubusercontent.com/86700191/160827088-a7c25459-090d-4728-8258-9a688a9f5a3c.png)

    이러한 과정으로 사용자가 정한 크기의 어휘집합이 될 때까지 반복수행한다. 이 떄 최종 어휘집합은 허깅페이스(huggingface) tokenizers 패키지에서 vocab.json 파일로 저장된다.
  그리고 병합된 바이그램 순서대로 병합 우선순위를 한눈에 볼 수 있게 merge.txt 파일로도 저장된다.<br><br>
  
  - BPE 토큰화 : 어휘 집합(vocab.json)과 병합 우선순위(merge.txt)를 이용하여 토큰화를 수행한다. 예를 들어 pug bug mug 라는 문장을 토큰화한다고 가정한다.<br><br>
  ![BPE02](https://user-images.githubusercontent.com/86700191/160835365-c31d0643-0d97-49bd-9829-871d6788eb3c.png)

    가장 먼저 토큰화를 수행하는 pug의 토큰화 과정은 다음과 같다.<br><br>
  ![BPE03](https://user-images.githubusercontent.com/86700191/160835372-f9fd49e0-ce49-46f4-9cb7-2e523182a1cc.png)

    이를 통해 pug의 토큰화 최종 결과는 p, ug라는 것을 알 수 있다. 같은 방법으로 bug, mug을 토큰화하면 다음과 같이 나누어진다.<br><br>
  ![BPE04](https://user-images.githubusercontent.com/86700191/160840388-45a281a2-40d5-434e-a0b3-6cb2180dfe61.png)
  
    결과를 보면 m이 &lt;unk&gt;로 토큰화 된 것을 볼 수 있다.  이것은 어휘집합에 m이 포함되어 있지 않아서 생기는 문제점이다. &lt;unk&gt;는 미등록 토큰(unknown token)을 의미한다. 
    하지만 일반적으로 알파벳과 같은 개별 문자들은 BPE 어휘집합을 구축할 때 초기에 포함되기 때문에 미등록 토큰이 될 경우는 적다. 위와 같은 경우는 가볍게 미등록 토큰이 어떻게 생기는가에 대한 예시라도 보면 된다.
<br><br>
- Wordpiece (워드피스) : 자주 등장한 문자열을 토큰으로 인식한다는 점에서 BPE와 본질적으로 유사하다 할 수 있지만 문자열을 병합하는 기준이 다르다. 워드피스는 빈도수가 아닌 가능도(likelihood)를 가장 높이는 쌍을 병합한다.<br><br>
  - 워드피스의 병합 기준 (병합 후보 a, b)<br><br>
  ![wordpiece](https://user-images.githubusercontent.com/86700191/161010262-4aba5703-b5d1-4691-8633-48e43125f6d8.png)
  
     분자는 ab가 연이어 등장할 확률, 분모는 a, b가 각각 등장할 확률의 곱이다. 이 값이 커지기 위해선 a, b가 서로 독립임을 가정했을 때보다 두 후보가 자주 동시에 등장해야 한다.
  따라서, 워드피스는 병합 후보에 오른 쌍을 미리 병합해보고 잃는 것과 가치 등을 판단하는 데에 위 기준 값을 이용하여 가장 높은 값을 가진 쌍을 병합한다. <br><br>
  허깅페이스(huggingface) tokenizers 패키지에서 워드피스기법은 어휘집합(vocab.json)만 가지고 토큰화한다. 분석 대상 어절에 어휘집합에 있는 서브워드가 포함 되어있는 경우 해당 서브워드를 분리시킨다.
  단, 서브워드 후보가 복수라면 가장 긴 서브워드를 선택하여 진행된다. 이후 이 과정을 반복 수행하며 토큰화를 진행한다. 만약 분석 대상 어절에 서브워드 후보가 없게 된다면 해당 어절 전체를 미등록 토큰으로 취급한다.